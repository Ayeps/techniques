{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk \n",
    "nltk.download('punkt')\n",
    "\n",
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = open(\"Airgas Inc2009.txt\").read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize, sent_tokenize, RegexpTokenizer\n",
    "\n",
    "sentences = sent_tokenize(text)\n",
    "words = word_tokenize(text)\n",
    "\n",
    "import nltk\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "input_file = \"Airgas Inc2009.txt\"\n",
    "words_file = \"words.txt\"\n",
    "output_file = \"output.txt\"\n",
    "curriculum_words = []\n",
    "pos_tagged_array = []\n",
    "base_words = []\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def process_words_file():\n",
    "    \"\"\"\n",
    "    Read the words file and store the words in a list\n",
    "    \"\"\"\n",
    "    global curriculum_words\n",
    "    curriculum_words = []\n",
    "    with open(words_file) as curriculum_file:\n",
    "        try:\n",
    "            for line in curriculum_file:\n",
    "                curriculum_words.append(line.strip())\n",
    "        except Exception as e:\n",
    "            print (e)\n",
    "\n",
    "\n",
    "def process_input():\n",
    "    \"\"\"\n",
    "    Read the input file and tokenize and POS_tag the words\n",
    "    \"\"\"\n",
    "    global pos_tagged_array\n",
    "    with open(input_file) as input_text:\n",
    "        try:\n",
    "            for line in input_text:\n",
    "                if line.strip():\n",
    "                    words = word_tokenize(line)\n",
    "                    for tag in nltk.pos_tag(words):\n",
    "                        # eliminating unnecessary POS tags\n",
    "                        match = re.search('\\w.*', tag[1])\n",
    "                        if match:\n",
    "                            pos_tagged_array.append(tag)\n",
    "        except Exception as e:\n",
    "            print (e)\n",
    "\n",
    "\n",
    "def lemmatize_words():\n",
    "    \"\"\"\n",
    "    Convert each word in the input to its base form\n",
    "    and save it in a list\n",
    "    \"\"\"\n",
    "    global base_words\n",
    "    for tag in pos_tagged_array:\n",
    "        base_word = tag[0].lower()\n",
    "        base_words.append(base_word)\n",
    "\n",
    "\n",
    "def analyze_input():\n",
    "    \"\"\"\n",
    "    Find count of words from the curriculum_words list\n",
    "    in the base_words list\n",
    "    \"\"\"\n",
    "    output = open(output_file, 'w')\n",
    "    for curriculum_word in curriculum_words:\n",
    "        count = base_words.count(curriculum_word)\n",
    "        output.write(\"%-15s | %10s\\n\" % (curriculum_word, str(count)))\n",
    "    output.close()\n",
    "\n",
    "\n",
    "process_words_file()\n",
    "process_input()\n",
    "lemmatize_words()\n",
    "analyze_input()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "words = \"cat dog child goose pants\"\n",
    "blob = TextBlob(ra)\n",
    "plurals = [word.pluralize() for word in blob.words]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py36",
   "language": "python",
   "name": "py36"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
