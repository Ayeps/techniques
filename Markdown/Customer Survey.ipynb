{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from lightgbm import LGBMClassifier\n",
    "from lightgbm import cv\n",
    "import gc\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "### What happens when you don't implement any predictions, but instead focus on non-linear interactions as your..\n",
    "### ..core form of analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Machine Learning Benchmarks\n",
    "## In linear models a PCA step can work nicely \n",
    "\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV, KFold\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, ExtraTreesRegressor\n",
    "from sklearn.svm import SVR, LinearSVR\n",
    "from sklearn.linear_model import ElasticNet, SGDRegressor, BayesianRidge\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.linear_model import Ridge, RidgeCV, ElasticNet, LassoCV, LassoLarsCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Structure Routine Loading and Holdout\n",
    "df = pd.read_excel(\"data/Satisfaction Survey.xlsx\")\n",
    "\n",
    "pickle.dump(df, open(\"data/df_airline.p\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "### to load holdout for preprocessing to be the same as model processing dataframe \n",
    "### df = pickle.load(open(\"holdout_airline.p\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Some regression problems have a low cardinality and seems like it might be easier to represent as a\n",
    "### classification problem, instead, I will stick with the regression narrative. \n",
    "### I am sure price sensitivity is a self reported measure as it is a survey. "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "### What predictor variables shows the strongest power in predicting:\n",
    "\n",
    "1. Departure Delays\n",
    "2. Arrival Delays\n",
    "3. Cancelations\n",
    "4. Satisfaction\n",
    "\n",
    "### Of course the majority of these prediction tasks are constrained by the data available, ..\n",
    "### the only true prediction excercise is predicting customer satisfaction. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.max_columns = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Satisfaction</th>\n",
       "      <th>Airline Status</th>\n",
       "      <th>Age</th>\n",
       "      <th>Age Range</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Price Sensitivity</th>\n",
       "      <th>Year of First Flight</th>\n",
       "      <th>No of Flights p.a.</th>\n",
       "      <th>No of Flights p.a. grouped</th>\n",
       "      <th>% of Flight with other Airlines</th>\n",
       "      <th>Type of Travel</th>\n",
       "      <th>No. of other Loyalty Cards</th>\n",
       "      <th>Shopping Amount at Airport</th>\n",
       "      <th>Eating and Drinking at Airport</th>\n",
       "      <th>Class</th>\n",
       "      <th>Day of Month</th>\n",
       "      <th>Flight date</th>\n",
       "      <th>Airline Code</th>\n",
       "      <th>Airline Name</th>\n",
       "      <th>Orgin City</th>\n",
       "      <th>Origin State</th>\n",
       "      <th>Destination City</th>\n",
       "      <th>Destination State</th>\n",
       "      <th>Scheduled Departure Hour</th>\n",
       "      <th>Departure Delay in Minutes</th>\n",
       "      <th>Arrival Delay in Minutes</th>\n",
       "      <th>Flight cancelled</th>\n",
       "      <th>Flight time in minutes</th>\n",
       "      <th>Flight Distance</th>\n",
       "      <th>Arrival Delay greater 5 Mins</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3.5</td>\n",
       "      <td>Gold</td>\n",
       "      <td>35</td>\n",
       "      <td>30-39</td>\n",
       "      <td>Male</td>\n",
       "      <td>1</td>\n",
       "      <td>2011</td>\n",
       "      <td>14.923291</td>\n",
       "      <td>11 to 20</td>\n",
       "      <td>5</td>\n",
       "      <td>Business travel</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>60</td>\n",
       "      <td>Eco</td>\n",
       "      <td>6</td>\n",
       "      <td>2014-03-06</td>\n",
       "      <td>MQ</td>\n",
       "      <td>EnjoyFlying Air Services</td>\n",
       "      <td>Madison, WI</td>\n",
       "      <td>Wisconsin</td>\n",
       "      <td>Dallas/Fort Worth, TX</td>\n",
       "      <td>Texas</td>\n",
       "      <td>6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>No</td>\n",
       "      <td>119.0</td>\n",
       "      <td>821</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>Platinum</td>\n",
       "      <td>46</td>\n",
       "      <td>40-49</td>\n",
       "      <td>Female</td>\n",
       "      <td>1</td>\n",
       "      <td>2012</td>\n",
       "      <td>28.800558</td>\n",
       "      <td>21 to 30</td>\n",
       "      <td>24</td>\n",
       "      <td>Business travel</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>75</td>\n",
       "      <td>Eco</td>\n",
       "      <td>15</td>\n",
       "      <td>2014-01-15</td>\n",
       "      <td>MQ</td>\n",
       "      <td>EnjoyFlying Air Services</td>\n",
       "      <td>Milwaukee, WI</td>\n",
       "      <td>Wisconsin</td>\n",
       "      <td>Dallas/Fort Worth, TX</td>\n",
       "      <td>Texas</td>\n",
       "      <td>10</td>\n",
       "      <td>13.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>No</td>\n",
       "      <td>114.0</td>\n",
       "      <td>853</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Blue</td>\n",
       "      <td>62</td>\n",
       "      <td>60-69</td>\n",
       "      <td>Male</td>\n",
       "      <td>1</td>\n",
       "      <td>2005</td>\n",
       "      <td>63.807531</td>\n",
       "      <td>61 to 70</td>\n",
       "      <td>8</td>\n",
       "      <td>Personal Travel</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>30</td>\n",
       "      <td>Eco</td>\n",
       "      <td>24</td>\n",
       "      <td>2014-01-24</td>\n",
       "      <td>MQ</td>\n",
       "      <td>EnjoyFlying Air Services</td>\n",
       "      <td>Milwaukee, WI</td>\n",
       "      <td>Wisconsin</td>\n",
       "      <td>Dallas/Fort Worth, TX</td>\n",
       "      <td>Texas</td>\n",
       "      <td>12</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>No</td>\n",
       "      <td>122.0</td>\n",
       "      <td>853</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>Blue</td>\n",
       "      <td>67</td>\n",
       "      <td>60-69</td>\n",
       "      <td>Female</td>\n",
       "      <td>1</td>\n",
       "      <td>2010</td>\n",
       "      <td>41.841004</td>\n",
       "      <td>41 to 50</td>\n",
       "      <td>5</td>\n",
       "      <td>Personal Travel</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>60</td>\n",
       "      <td>Eco</td>\n",
       "      <td>6</td>\n",
       "      <td>2014-03-06</td>\n",
       "      <td>MQ</td>\n",
       "      <td>EnjoyFlying Air Services</td>\n",
       "      <td>Milwaukee, WI</td>\n",
       "      <td>Wisconsin</td>\n",
       "      <td>Dallas/Fort Worth, TX</td>\n",
       "      <td>Texas</td>\n",
       "      <td>9</td>\n",
       "      <td>12.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>No</td>\n",
       "      <td>127.0</td>\n",
       "      <td>853</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Blue</td>\n",
       "      <td>44</td>\n",
       "      <td>40-49</td>\n",
       "      <td>Female</td>\n",
       "      <td>1</td>\n",
       "      <td>2003</td>\n",
       "      <td>12.552301</td>\n",
       "      <td>11 to 20</td>\n",
       "      <td>1</td>\n",
       "      <td>Business travel</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>90</td>\n",
       "      <td>Business</td>\n",
       "      <td>20</td>\n",
       "      <td>2014-03-20</td>\n",
       "      <td>MQ</td>\n",
       "      <td>EnjoyFlying Air Services</td>\n",
       "      <td>Madison, WI</td>\n",
       "      <td>Wisconsin</td>\n",
       "      <td>Chicago, IL</td>\n",
       "      <td>Illinois</td>\n",
       "      <td>9</td>\n",
       "      <td>6.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>No</td>\n",
       "      <td>30.0</td>\n",
       "      <td>108</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Satisfaction Airline Status  Age Age Range  Gender  Price Sensitivity  \\\n",
       "0          3.5           Gold   35     30-39    Male                  1   \n",
       "1            5       Platinum   46     40-49  Female                  1   \n",
       "2            2           Blue   62     60-69    Male                  1   \n",
       "3            1           Blue   67     60-69  Female                  1   \n",
       "4            4           Blue   44     40-49  Female                  1   \n",
       "\n",
       "   Year of First Flight  No of Flights p.a. No of Flights p.a. grouped  \\\n",
       "0                  2011           14.923291                   11 to 20   \n",
       "1                  2012           28.800558                   21 to 30   \n",
       "2                  2005           63.807531                   61 to 70   \n",
       "3                  2010           41.841004                   41 to 50   \n",
       "4                  2003           12.552301                   11 to 20   \n",
       "\n",
       "   % of Flight with other Airlines   Type of Travel  \\\n",
       "0                                5  Business travel   \n",
       "1                               24  Business travel   \n",
       "2                                8  Personal Travel   \n",
       "3                                5  Personal Travel   \n",
       "4                                1  Business travel   \n",
       "\n",
       "   No. of other Loyalty Cards  Shopping Amount at Airport  \\\n",
       "0                           0                           0   \n",
       "1                           1                           0   \n",
       "2                           0                           0   \n",
       "3                           0                           0   \n",
       "4                           0                           0   \n",
       "\n",
       "   Eating and Drinking at Airport     Class  Day of Month Flight date  \\\n",
       "0                              60       Eco             6  2014-03-06   \n",
       "1                              75       Eco            15  2014-01-15   \n",
       "2                              30       Eco            24  2014-01-24   \n",
       "3                              60       Eco             6  2014-03-06   \n",
       "4                              90  Business            20  2014-03-20   \n",
       "\n",
       "  Airline Code              Airline Name     Orgin City Origin State  \\\n",
       "0           MQ  EnjoyFlying Air Services    Madison, WI    Wisconsin   \n",
       "1           MQ  EnjoyFlying Air Services  Milwaukee, WI    Wisconsin   \n",
       "2           MQ  EnjoyFlying Air Services  Milwaukee, WI    Wisconsin   \n",
       "3           MQ  EnjoyFlying Air Services  Milwaukee, WI    Wisconsin   \n",
       "4           MQ  EnjoyFlying Air Services    Madison, WI    Wisconsin   \n",
       "\n",
       "        Destination City Destination State  Scheduled Departure Hour  \\\n",
       "0  Dallas/Fort Worth, TX             Texas                         6   \n",
       "1  Dallas/Fort Worth, TX             Texas                        10   \n",
       "2  Dallas/Fort Worth, TX             Texas                        12   \n",
       "3  Dallas/Fort Worth, TX             Texas                         9   \n",
       "4            Chicago, IL          Illinois                         9   \n",
       "\n",
       "   Departure Delay in Minutes  Arrival Delay in Minutes Flight cancelled  \\\n",
       "0                         0.0                       0.0               No   \n",
       "1                        13.0                       0.0               No   \n",
       "2                         0.0                       0.0               No   \n",
       "3                        12.0                       1.0               No   \n",
       "4                         6.0                      24.0               No   \n",
       "\n",
       "   Flight time in minutes  Flight Distance Arrival Delay greater 5 Mins  \n",
       "0                   119.0              821                           no  \n",
       "1                   114.0              853                           no  \n",
       "2                   122.0              853                           no  \n",
       "3                   127.0              853                           no  \n",
       "4                    30.0              108                          yes  "
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fits should happen on the train and be applied on test and train\n",
    "df = pickle.load(open(\"data/df_airline.p\", \"rb\"))\n",
    "np.random.seed(10)\n",
    "\n",
    "df.loc[38897,\"Satisfaction\"] = 4\n",
    "df.loc[38898,\"Satisfaction\"] = 4\n",
    "df.loc[38899,\"Satisfaction\"] = 4\n",
    "\n",
    "df[\"Satisfaction\"] = pd.to_numeric(df[\"Satisfaction\"])\n",
    "\n",
    "import numpy as np\n",
    "#Keeping 15% for the validation set, thats about 1000 instances to test on\n",
    "\n",
    "from sklearn import preprocessing\n",
    "le = preprocessing.LabelEncoder()\n",
    "\n",
    "for col in df:\n",
    "    if df[col].dtype == 'object':\n",
    "        df[col] = le.fit_transform(df[col])\n",
    "\n",
    "msk = np.random.rand(len(df)) < 0.30\n",
    "df[\"is_test\"] = msk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dereksnow/anaconda/envs/py36/lib/python3.6/site-packages/ipykernel/__main__.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "X_train = df[df[\"is_test\"]==False].drop(col_drop,axis=1)\n",
    "y_train = df[df[\"is_test\"]==False][\"Satisfaction\"]\n",
    "\n",
    "test_df = df.loc[df.is_test, :]\n",
    "\n",
    "msk = np.random.rand(len(test_df)) < 0.50\n",
    "test_df[\"is_holdout\"] = msk\n",
    "holdout = test_df[test_df[\"is_holdout\"]==True].reset_index(drop=True).drop(\"is_holdout\",axis=1)\n",
    "test_df = test_df[test_df[\"is_holdout\"]==False].reset_index(drop=True).drop(\"is_holdout\",axis=1)\n",
    "\n",
    "X_test = test_df.drop(col_drop, axis=1)\n",
    "y_test = test_df[\"Satisfaction\"]\n",
    "\n",
    "X_hold = holdout.drop(col_drop, axis=1)\n",
    "y_hold = holdout[\"Satisfaction\"]\n",
    "del holdout\n",
    "\n",
    "X_full = pd.concat((df[df[\"is_test\"]==False], test_df),axis=0).drop(col_drop, axis=1)\n",
    "y_full = pd.concat((df[df[\"is_test\"]==False], test_df),axis=0)[\"Satisfaction\"]\n",
    "\n",
    "d_train = lgb.Dataset(X_train, label=y_train)\n",
    "d_test = lgb.Dataset(X_test, label=y_test)\n",
    "d_hold = lgb.Dataset(X_hold, label=y_hold)\n",
    "#d_test = lgb.Dataset(X_test, label=y_test.sample(len(y_test)).reset_index(drop=True)) ## quick bench"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(110343,)"
      ]
     },
     "execution_count": 414,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_full.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(110343, 29)"
      ]
     },
     "execution_count": 415,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_full.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [],
   "source": [
    "### I doubt these parameters can be improved further - the gap between training and valid has decreased.\n",
    "### learning curves should not be used to jusde variance and bias - instead to see if more data will help.\n",
    "params = {\n",
    "    'boosting_type': 'gbdt',\n",
    "    'objective': 'regression',\n",
    "    'metric': 'mae',\n",
    "    'subsample':.80,\n",
    "    'num_leaves':50,\n",
    "    \"seed\":10,\n",
    "    'min_data_in_leaf':180,\n",
    "    'feature_fraction':.80,\n",
    "    \"max_bin\":100,\n",
    "    'max_depth': 6, \n",
    "    'learning_rate': 0.1,\n",
    "    'verbose': 0, }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fast CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dereksnow/anaconda/envs/py36/lib/python3.6/site-packages/lightgbm/engine.py:99: UserWarning: Found `num_boost_round` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 103 rounds.\n",
      "[100]\ttraining's l1: 0.505691\tvalid_1's l1: 0.51388\n",
      "Early stopping, best iteration is:\n",
      "[78]\ttraining's l1: 0.507267\tvalid_1's l1: 0.513656\n"
     ]
    }
   ],
   "source": [
    "## Best Score\n",
    "## Only difference is - this is quick and dirty (Rapid Testing)\n",
    "\n",
    "## There is a gap between validation and training, so it is overfitting\n",
    "## Automated is not necessary if you know your parameters - too much of a hastle\n",
    "## Do anything you can to improve validtion score\n",
    "## Remember CV is only there for parameter optimisation and that is what you are using it for.\n",
    "## This is just normal validation and not folder validation\n",
    "model = lgb.train(params,d_train, num_boost_round=10000,valid_sets=[d_train, d_test], early_stopping_rounds=103,verbose_eval=100,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Medium CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20]\tcv_agg's l1: 0.531576 + 0.00124649\n",
      "[40]\tcv_agg's l1: 0.51668 + 0.00128588\n",
      "[60]\tcv_agg's l1: 0.5159 + 0.00137973\n",
      "[80]\tcv_agg's l1: 0.516048 + 0.00141066\n",
      "[100]\tcv_agg's l1: 0.516196 + 0.00134864\n",
      "[120]\tcv_agg's l1: 0.516425 + 0.00133711\n",
      "[140]\tcv_agg's l1: 0.516672 + 0.00123967\n",
      "[160]\tcv_agg's l1: 0.516859 + 0.00126381\n",
      "[180]\tcv_agg's l1: 0.517057 + 0.00117263\n",
      "[200]\tcv_agg's l1: 0.517354 + 0.00125141\n",
      "[220]\tcv_agg's l1: 0.517525 + 0.00128193\n",
      "score:  0.5158844396077821\n"
     ]
    }
   ],
   "source": [
    "## Medium Score\n",
    "## Only difference is - this takes longer but more accurate (Medium Testing)\n",
    "\n",
    "\n",
    "## Holdout set is (long form testing)\n",
    "\n",
    "from lightgbm import cv\n",
    "from lightgbm import Dataset\n",
    "\n",
    "## Here you don't have to provide a validation set as it will cross validae\n",
    "def get_score(X, y, usecols, params,seeder,depther, verbose =20):  \n",
    "     dtrain = Dataset(X[usecols], y)\n",
    "     # params[\"max_depth\"] = depther\n",
    "\n",
    "     eval =  cv(params,\n",
    "             dtrain,\n",
    "             nfold=3,\n",
    "             num_boost_round=20000,\n",
    "             early_stopping_rounds=160, ## After it stopped how long should go on. \n",
    "             verbose_eval=verbose,\n",
    "             stratified=False,  ## by default it is true, so have to set to false\n",
    "             #verbose_eval=-1,\n",
    "             seed = seeder,\n",
    "             show_stdv=True)\n",
    "     #print(eval)\n",
    "     return eval, min(eval['l1-mean']), eval[\"l1-mean\"].index(min(eval[\"l1-mean\"]))\n",
    "  \n",
    "eval, score, seed_best = get_score(X_full,y_full ,list(X_full.columns), params, seeder=10,depther=6)\n",
    "print(\"score: \", score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensemble CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score:  0.5160619578901716\n"
     ]
    }
   ],
   "source": [
    "## Worst Score\n",
    "## This is a submission CV to identify iterations for the different parameter elections of chosen\n",
    "## ensemble models. \n",
    "\n",
    "random_seeds = [5, 1, 9 ,12, 20]\n",
    "depths = [6,6,6,6,6]\n",
    "\n",
    "seed_dict = {}\n",
    "final_score = 0\n",
    "for seedling, deepling in zip(random_seeds,depths ):  ## Here is where you add parameter adaptions\n",
    "    eval, score, seed_best = get_score(X_full,y_full ,list(X_full.columns), params, seeder=seedling,depther=deepling, verbose=-20)\n",
    "    final_score += score\n",
    "    seed_dict[seedling] = seed_best\n",
    "print(\"score: \", final_score/5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fast Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5135152460923419"
      ]
     },
     "execution_count": 419,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = model.predict(X_hold)\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "mean_absolute_error(y_hold, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(110343, 29)\n",
      "(88274, 29)\n",
      "itteration:  100\n",
      "Fold 1 MAE: 0.51615027\n",
      "(110343, 29)\n",
      "(88274, 29)\n",
      "itteration:  100\n",
      "Fold 2 MAE: 0.51819584\n",
      "(110343, 29)\n",
      "(88274, 29)\n",
      "itteration:  100\n",
      "Fold 3 MAE: 0.51731088\n",
      "(110343, 29)\n",
      "(88275, 29)\n",
      "itteration:  100\n",
      "Fold 4 MAE: 0.51331907\n",
      "(110343, 29)\n",
      "(88275, 29)\n",
      "itteration:  100\n",
      "Fold 5 MAE: 0.51443229\n",
      "In Sample MAE score 0.51588171\n",
      "Out of sample MAE:  0.5133541335204654\n",
      "avg iteration:  100.0\n"
     ]
    }
   ],
   "source": [
    "n_splits = 5\n",
    "cvv = KFold(n_splits=n_splits, random_state=42)\n",
    "oof_preds = np.zeros(X_full.shape[0])\n",
    "\n",
    "sub = y_hold.to_frame()\n",
    "\n",
    "sub[\"Target\"] = 0\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "feature_importances = pd.DataFrame()\n",
    "avg_iter = 0\n",
    "\n",
    "for i, (fit_idx, val_idx) in enumerate(cvv.split(X_full, y_full)):\n",
    "\n",
    "    X_fit = X_full.iloc[fit_idx]\n",
    "    y_fit = y_full.iloc[fit_idx]\n",
    "    X_val = X_full.iloc[val_idx]\n",
    "    y_val = y_full.iloc[val_idx]\n",
    "    print(X_full.shape)\n",
    "    print(X_fit.shape)\n",
    "    \n",
    "    model = LGBMRegressor(\n",
    "**params\n",
    "    )\n",
    "\n",
    "    model.fit(\n",
    "        X_fit,\n",
    "        y_fit,\n",
    "        eval_set=[(X_fit, y_fit), (X_val, y_val)],\n",
    "        eval_names=('fit', 'val'),\n",
    "        eval_metric='mae',\n",
    "        early_stopping_rounds=150,\n",
    "        verbose=False\n",
    "    )\n",
    "    \n",
    "    print(\"itteration: \", model.best_iteration_)\n",
    "    avg_iter += model.best_iteration_\n",
    "    oof_preds[val_idx] = model.predict(X_val, num_iteration=model.best_iteration_)\n",
    "    sub['Target'] += model.predict(X_hold, num_iteration=model.best_iteration_)\n",
    "    \n",
    "    fi = pd.DataFrame()\n",
    "    fi[\"feature\"] = X_full.columns\n",
    "    fi[\"importance\"] = model.feature_importances_\n",
    "    fi[\"fold\"] = (i+1)\n",
    "    \n",
    "    feature_importances = pd.concat([feature_importances, fi], axis=0)\n",
    "    \n",
    "    print(\"Fold {} MAE: {:.8f}\".format(i+1, mean_absolute_error(y_val, oof_preds[val_idx])))\n",
    "\n",
    "print('In Sample MAE score %.8f' % mean_absolute_error(y_full, oof_preds)) \n",
    "print('Out of sample MAE: ', mean_absolute_error(y_hold, sub['Target']/n_splits))\n",
    "print(\"avg iteration: \",(avg_iter/n_splits))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Naturally you expect this one to perform somewhat better and ineed it does\n",
    "## No rediculous improvement, but improvement all the same."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensemble Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 66, 5: 50, 9: 50, 12: 50, 20: 67}"
      ]
     },
     "execution_count": 421,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seed_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 391,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_seeds[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 386,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Everything worked out exactely as I thought. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dereksnow/anaconda/envs/py36/lib/python3.6/site-packages/lightgbm/engine.py:99: UserWarning: Found `num_boost_round` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dereksnow/anaconda/envs/py36/lib/python3.6/site-packages/lightgbm/engine.py:99: UserWarning: Found `num_boost_round` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dereksnow/anaconda/envs/py36/lib/python3.6/site-packages/lightgbm/engine.py:99: UserWarning: Found `num_boost_round` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dereksnow/anaconda/envs/py36/lib/python3.6/site-packages/lightgbm/engine.py:99: UserWarning: Found `num_boost_round` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dereksnow/anaconda/envs/py36/lib/python3.6/site-packages/lightgbm/engine.py:99: UserWarning: Found `num_boost_round` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out of sample MAE:  0.5129196662064283\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "sub = y_hold.to_frame()\n",
    "sub[\"Target\"] = 0\n",
    "\n",
    "feature_importances = pd.DataFrame()\n",
    "avg_iter = 0\n",
    "\n",
    "ba= -1 \n",
    "for i, r in enumerate(range(n_splits)):\n",
    "    print(i)\n",
    "    ba = ba + 1\n",
    "    \n",
    "    params[\"num_boost_round\"] = int(seed_dict[random_seeds[ba]]*1.1)\n",
    "    params[\"seed\"] = random_seeds[ba]\n",
    "    params[\"max_depth\"] = depths[ba] # if you want to customise uncomment\n",
    "    \n",
    "    model = LGBMRegressor(\n",
    "**params\n",
    "    )\n",
    "\n",
    "    model.fit(\n",
    "        X_full,\n",
    "        y_full,\n",
    "        eval_metric='mae',\n",
    "        verbose=False\n",
    "    )\n",
    "    \n",
    "\n",
    "    sub['Target'] += model.predict(X_hold)\n",
    "    \n",
    "    fi = pd.DataFrame()\n",
    "    fi[\"feature\"] = X_full.columns\n",
    "    fi[\"importance\"] = model.feature_importances_\n",
    "    fi[\"fold\"] = (i+1)\n",
    "    \n",
    "    feature_importances = pd.concat([feature_importances, fi], axis=0)\n",
    "    \n",
    "print('Out of sample MAE: ', mean_absolute_error(y_hold, sub['Target']/n_splits))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1.1 Out of sample MAE:  0.5129196662064283\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dereksnow/anaconda/envs/py36/lib/python3.6/site-packages/lightgbm/engine.py:99: UserWarning: Found `num_boost_round` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dereksnow/anaconda/envs/py36/lib/python3.6/site-packages/lightgbm/engine.py:99: UserWarning: Found `num_boost_round` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dereksnow/anaconda/envs/py36/lib/python3.6/site-packages/lightgbm/engine.py:99: UserWarning: Found `num_boost_round` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dereksnow/anaconda/envs/py36/lib/python3.6/site-packages/lightgbm/engine.py:99: UserWarning: Found `num_boost_round` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dereksnow/anaconda/envs/py36/lib/python3.6/site-packages/lightgbm/engine.py:99: UserWarning: Found `num_boost_round` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out of sample MAE:  0.5064183766351881\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "sub = y_hold.to_frame()\n",
    "sub[\"Target\"] = 0\n",
    "\n",
    "feature_importances = pd.DataFrame()\n",
    "avg_iter = 0\n",
    "\n",
    "ba= -1 \n",
    "for i, r in enumerate(range(n_splits)):\n",
    "    print(i)\n",
    "    ba = ba + 1\n",
    "    \n",
    "    params[\"num_boost_round\"] = seed_dict[random_seeds[ba]]  \n",
    "    params[\"seed\"] = random_seeds[ba]\n",
    "    params[\"max_depth\"] = depths[ba] # if you want to customise uncomment\n",
    "    \n",
    "    model = LGBMRegressor(\n",
    "**params\n",
    "    )\n",
    "\n",
    "    model.fit(\n",
    "        X_full,\n",
    "        y_full,\n",
    "        eval_metric='mae',\n",
    "        verbose=False\n",
    "    )\n",
    "    \n",
    "\n",
    "    sub['Target'] += model.predict(X_hold)\n",
    "    \n",
    "    fi = pd.DataFrame()\n",
    "    fi[\"feature\"] = X_full.columns\n",
    "    fi[\"importance\"] = model.feature_importances_\n",
    "    fi[\"fold\"] = (i+1)\n",
    "    \n",
    "    feature_importances = pd.concat([feature_importances, fi], axis=0)\n",
    "    \n",
    "print('Out of sample MAE: ', mean_absolute_error(y_hold, sub['Target']/n_splits))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "mean_absolute_error(y_hold, y_pred)\n",
    "# 0.5163493564074798"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5155678000111037"
      ]
     },
     "execution_count": 291,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval['l1-mean'][seed_best]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import learning_curve\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "\n",
    "\n",
    "def plot_learning_curve(estimator, title, X, y, ylim=None, cv=None,\n",
    "                        n_jobs=1, train_sizes=np.linspace(.1, 1.0, 5)):\n",
    "    \"\"\"\n",
    "    Generate a simple plot of the test and training learning curve.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    estimator : object type that implements the \"fit\" and \"predict\" methods\n",
    "        An object of that type which is cloned for each validation.\n",
    "\n",
    "    title : string\n",
    "        Title for the chart.\n",
    "\n",
    "    X : array-like, shape (n_samples, n_features)\n",
    "        Training vector, where n_samples is the number of samples and\n",
    "        n_features is the number of features.\n",
    "\n",
    "    y : array-like, shape (n_samples) or (n_samples, n_features), optional\n",
    "        Target relative to X for classification or regression;\n",
    "        None for unsupervised learning.\n",
    "\n",
    "    ylim : tuple, shape (ymin, ymax), optional\n",
    "        Defines minimum and maximum yvalues plotted.\n",
    "\n",
    "    cv : int, cross-validation generator or an iterable, optional\n",
    "        Determines the cross-validation splitting strategy.\n",
    "        Possible inputs for cv are:\n",
    "          - None, to use the default 3-fold cross-validation,\n",
    "          - integer, to specify the number of folds.\n",
    "          - An object to be used as a cross-validation generator.\n",
    "          - An iterable yielding train/test splits.\n",
    "\n",
    "        For integer/None inputs, if ``y`` is binary or multiclass,\n",
    "        :class:`StratifiedKFold` used. If the estimator is not a classifier\n",
    "        or if ``y`` is neither binary nor multiclass, :class:`KFold` is used.\n",
    "\n",
    "        Refer :ref:`User Guide <cross_validation>` for the various\n",
    "        cross-validators that can be used here.\n",
    "\n",
    "    n_jobs : integer, optional\n",
    "        Number of jobs to run in parallel (default 1).\n",
    "    \"\"\"\n",
    "    plt.figure()\n",
    "    plt.title(title)\n",
    "    if ylim is not None:\n",
    "        plt.ylim(*ylim)\n",
    "    plt.xlabel(\"Training examples\")\n",
    "    plt.ylabel(\"Score\")\n",
    "    train_sizes, train_scores, test_scores = learning_curve(\n",
    "        estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)\n",
    "    train_scores_mean = np.mean(train_scores, axis=1)\n",
    "    train_scores_std = np.std(train_scores, axis=1)\n",
    "    test_scores_mean = np.mean(test_scores, axis=1)\n",
    "    test_scores_std = np.std(test_scores, axis=1)\n",
    "    plt.grid()\n",
    "\n",
    "    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n",
    "                     train_scores_mean + train_scores_std, alpha=0.1,\n",
    "                     color=\"r\")\n",
    "    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n",
    "                     test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n",
    "    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n",
    "             label=\"Training score\")\n",
    "    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n",
    "             label=\"Cross-validation score\")\n",
    "\n",
    "    plt.legend(loc=\"best\")\n",
    "    return plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## There is no purpose for CV, I have enough data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dereksnow/anaconda/envs/py36/lib/python3.6/site-packages/lightgbm/engine.py:99: UserWarning: Found `num_boost_round` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/Users/dereksnow/anaconda/envs/py36/lib/python3.6/site-packages/lightgbm/engine.py:99: UserWarning: Found `num_boost_round` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/Users/dereksnow/anaconda/envs/py36/lib/python3.6/site-packages/lightgbm/engine.py:99: UserWarning: Found `num_boost_round` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/Users/dereksnow/anaconda/envs/py36/lib/python3.6/site-packages/lightgbm/engine.py:99: UserWarning: Found `num_boost_round` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/Users/dereksnow/anaconda/envs/py36/lib/python3.6/site-packages/lightgbm/engine.py:99: UserWarning: Found `num_boost_round` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/Users/dereksnow/anaconda/envs/py36/lib/python3.6/site-packages/lightgbm/engine.py:99: UserWarning: Found `num_boost_round` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/Users/dereksnow/anaconda/envs/py36/lib/python3.6/site-packages/lightgbm/engine.py:99: UserWarning: Found `num_boost_round` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/Users/dereksnow/anaconda/envs/py36/lib/python3.6/site-packages/lightgbm/engine.py:99: UserWarning: Found `num_boost_round` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/Users/dereksnow/anaconda/envs/py36/lib/python3.6/site-packages/lightgbm/engine.py:99: UserWarning: Found `num_boost_round` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/Users/dereksnow/anaconda/envs/py36/lib/python3.6/site-packages/lightgbm/engine.py:99: UserWarning: Found `num_boost_round` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/Users/dereksnow/anaconda/envs/py36/lib/python3.6/site-packages/lightgbm/engine.py:99: UserWarning: Found `num_boost_round` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/Users/dereksnow/anaconda/envs/py36/lib/python3.6/site-packages/lightgbm/engine.py:99: UserWarning: Found `num_boost_round` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/Users/dereksnow/anaconda/envs/py36/lib/python3.6/site-packages/lightgbm/engine.py:99: UserWarning: Found `num_boost_round` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/Users/dereksnow/anaconda/envs/py36/lib/python3.6/site-packages/lightgbm/engine.py:99: UserWarning: Found `num_boost_round` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/Users/dereksnow/anaconda/envs/py36/lib/python3.6/site-packages/lightgbm/engine.py:99: UserWarning: Found `num_boost_round` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<module 'matplotlib.pyplot' from '/Users/dereksnow/anaconda/envs/py36/lib/python3.6/site-packages/matplotlib/pyplot.py'>"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYsAAAEWCAYAAACXGLsWAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a172135c0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "params[\"num_boost_round\"] = 72\n",
    "### They have converged enough, what this shows is that their is enough data in this task more data != higher score\n",
    "plot_learning_curve(LGBMRegressor(**params), \"Learning\", df.drop(col_drop,axis=1), df[\"Satisfaction\"], ylim=None, cv=None,\n",
    "                        n_jobs=1, train_sizes=np.linspace(.1, 1.0, 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tl;dr when the score is lower than it should be the bias is an issue it is underfitted, when the score is good for training but bad for validation curve then it is overfitting and has too much variance.\n",
    "\n",
    "Firstly, we should focus on the right side of the plot, where there are sufficient data for evaluation.\n",
    "\n",
    "    If two curves are \"close to each other\" and both of them but have a low score. The model suffer from an under fitting problem (High Bias)\n",
    "\n",
    "    If training curve has a much better score but testing curve has a lower score, i.e., there are large gaps between two curves. Then the model suffer from an over fitting problem (High Variance)\n",
    "\n",
    "\n",
    "All that has to happen is the cross validation score has to increase and eventually somewhat converge oveer time. If it doesn't you are overfitting. \n",
    "\n",
    "I think what you're seeing is normal behaviour:\n",
    "\n",
    "With only few samples (like 2000) it's easy for a model to (over)fit the data - but it doesn't generalize well. So you get high training accuracy, but the model might not work well with new data (i.e. low validation/test accuracy).\n",
    "\n",
    "As you add more samples (like 9000) it becomes harder for the model to fit the data - so you get a lower training accuracy, but the model will work better with new data (i.e. validation/test accuracy starts to rise).\n",
    "\n",
    "So:\n",
    "\n",
    "    As the training dataset increases, the training accuracy is supposed to decrease because more data is harder to fit well.\n",
    "\n",
    "    As the training dataset increases, the validation/test accuracy is supposed to increase as well since less overfitting means better generalization.\n",
    "\n",
    "Andrew Ng has a video about learning curves. Note that he plots the error on the y axis, you have the accuracy on the y axis.. so the y axis is flipped.\n",
    "\n",
    "Also take a look at the second half of the video. It explains high bias and high variance problems.\n",
    "\n",
    "Your model seems to have high variance (due to the big \"gap\" between the two curves) - it's still too complex for the small amount of data you've got. Either getting more data or using a simpler model (or more regularization on the same model) might improve the results.\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#First of all, your training accuracy goes down quite a bit when you add more examples. So this could still be high variance. However, I doubt that this is the only explanation as the gap seems to be too big.\n",
    "# I understand that the gap between training and testing score is probably due to high variance(overfitting). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Predicting, mean, median, mode, logistic and a groupby"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics\n",
    "Bring in the holdoutset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5170065010722477"
      ]
     },
     "execution_count": 258,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = model.predict(X_hold)\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "mean_absolute_error(y_hold, y_pred)\n",
    "# 0.5163493564074798"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train\n",
      "Mean:  0.8252751318191531 Median:  0.8038245451265399 Mode:  0.8038245451265399 Random 1.0412092160197928\n",
      "Test Peaking\n",
      "Mean:  0.8248216444238886 Median:  0.8038245451265399 Mode:  0.8038245451265399 Random 1.0403587443946187\n"
     ]
    }
   ],
   "source": [
    "## Standard Benchmarks\n",
    "\n",
    "y_mean = y_test.to_frame()\n",
    "y_mean[\"Satisfaction\"] = y_train.mean()\n",
    "\n",
    "\n",
    "y_median = y_test.to_frame()\n",
    "y_median[\"Satisfaction\"] = y_train.median()\n",
    "\n",
    "y_mode = y_test.to_frame()\n",
    "y_mode[\"Satisfaction\"] = y_train.mode().values[0]\n",
    "\n",
    "\n",
    "print(\"Train\")\n",
    "print(\"Mean: \",mean_absolute_error(y_test, y_mean),\n",
    "      \"Median: \",mean_absolute_error(y_test, y_median),\n",
    "      \"Mode: \",mean_absolute_error(y_test, y_mode),\n",
    "      \"Random\",mean_absolute_error(y_test, y_train.sample(len(y_test)))) \n",
    "\n",
    "## Mean Test Distribution Peaking Naive Benchmark (TDPNB)\n",
    "y_mean = y_test.to_frame()\n",
    "y_mean[\"Satisfaction\"] = y_test.mean()\n",
    "\n",
    "\n",
    "# Median TDBPN\n",
    "y_median = y_test.to_frame()\n",
    "y_median[\"Satisfaction\"] = y_test.median()\n",
    "\n",
    "## Mode TDPNB\n",
    "y_mode = y_test.to_frame()\n",
    "y_mode[\"Satisfaction\"] = y_test.mode().values[0]\n",
    "    \n",
    "print(\"Test Peaking\")\n",
    "print(\"Mean: \",mean_absolute_error(y_test, y_mean),\n",
    "      \"Median: \",mean_absolute_error(y_test, y_median),\n",
    "      \"Mode: \",mean_absolute_error(y_test, y_mode),\n",
    "      \"Random\",mean_absolute_error(y_test, y_test.sample(len(y_test)))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "## This is going to give Laso some light. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error_triggered\n",
      "error_triggered\n"
     ]
    }
   ],
   "source": [
    "def rmse_cv(model,X,y):\n",
    "    rmse = np.sqrt(-cross_val_score(model, X, y, scoring=\"neg_mean_squared_error\", cv=5))\n",
    "    return rmse\n",
    "\n",
    "\n",
    "def Standardisation(X_train, X_test, leave_out=None):\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    if leave_out:\n",
    "        leave = X_train[leave_out]\n",
    "        X_train = X_train.drop(leave_out,axis=1)\n",
    "\n",
    "        listed = list(X_train)\n",
    "        scaler = StandardScaler()\n",
    "        try:\n",
    "            X_train = scaler.fit_transform(X_train)\n",
    "        except:\n",
    "            print(\"error_triggered\")\n",
    "            X_train = scaler.fit_transform(X_train.fillna(X_train.mean()))\n",
    "            X_test = X_test.fillna(X_test.mean())\n",
    "        \n",
    "        X_test = scaler.transform(X_test)\n",
    "        X_train = pd.DataFrame(X_train)\n",
    "        X_test = pd.DataFrame(X_test)\n",
    "        X_train.columns = listed\n",
    "        X_train = pd.concat((X_train,leave ),axis=1)\n",
    "        X_test.columns = listed\n",
    "        X_test = pd.concat((X_test,leave ),axis=1)\n",
    "    else:\n",
    "        scaler = StandardScaler()\n",
    "        listed = list(X_train)\n",
    "        try:\n",
    "            X_train = scaler.fit_transform(X_train)\n",
    "        except:\n",
    "            print(\"error_triggered\")\n",
    "            X_train = scaler.fit_transform(X_train.fillna(X_train.mean()))\n",
    "            X_test = X_test.fillna(X_test.mean())\n",
    "        X_test = scaler.transform(X_test)\n",
    "        X_test = pd.DataFrame(X_test)\n",
    "        X_test.columns = listed\n",
    "        \n",
    "        X_train = pd.DataFrame(X_train)\n",
    "        X_train.columns = listed\n",
    "        \n",
    "    return X_train, X_test\n",
    "\n",
    "X_train_s, X_test_s = Standardisation(X_train,X_test,[])\n",
    "_, X_hold_s = Standardisation(X_train,X_hold,[])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "## This is a quick and dirty way to do cs for lasso without plotting, but I want to plot the alphas, see below.\n",
    "## Alpha is simply the regularisation term\n",
    "model_lasso = LassoCV(alphas = [1, 0.1, 0.001, 0.0005]).fit(X_train_s, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we'll run a ridge regression and see how score varies with different alphas. This will show how picking a different alpha score changes the R2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, α (alpha) is the parameter which balances the amount of emphasis given to minimizing RSS vs minimizing sum of square of coefficients. α can take various values:\n",
    "\n",
    "    α = 0:\n",
    "        The objective becomes same as simple linear regression.\n",
    "        We’ll get the same coefficients as simple linear regression.\n",
    "    α = ∞:\n",
    "        The coefficients will be zero. Why? Because of infinite weightage on square of coefficients, anything less than zero will make the objective infinite.\n",
    "    0 < α < ∞:\n",
    "        The magnitude of α will decide the weightage given to different parts of objective.\n",
    "        The coefficients will be somewhere between 0 and ones for simple linear regression.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svr = SVR(gamma= 0.0004,kernel='rbf',C=13,epsilon=0.009)\n",
    "#ker = KernelRidge(alpha=0.2 ,kernel='polynomial',degree=3 , coef0=0.8) ## too computationally expensive\n",
    "ela = ElasticNet(alpha=0.005,l1_ratio=0.08,max_iter=10000)\n",
    "bay = BayesianRidge()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5006963184228322"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## This is a great score without hyper-parametr tampering. \n",
    "svr = ElasticNet(alpha=0.005,l1_ratio=0.08,max_iter=10000).fit(X_train_s, y_train)\n",
    "y_svr = svr.predict(X_test_s)\n",
    "mean_absolute_error(y_test, y_svr)\n",
    "# 0.5006963184228322"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ela = SVR(gamma= 0.0004,kernel='rbf',C=13,epsilon=0.009).fit(X_train_s, y_train)\n",
    "y_ela = ela.predict(X_test_s)\n",
    "mean_absolute_error(y_test, y_ela)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bay = BayesianRidge().fit(X_train_s, y_train)\n",
    "y_bay = bay.predict(X_test_s)\n",
    "mean_absolute_error(y_test, y_bay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_kernel_ridge = [] \n",
    "alpha_space = np.logspace(-1, 0, 3)\n",
    "for r in alpha_space:\n",
    "    kr = KernelRidge(alpha= r, kernel=\"polynomial\", degree=3).fit(X_train_s, y_train)\n",
    "\n",
    "    y_kr = kr.predict(X_test_s)\n",
    "\n",
    "    score_kernel_ridge.append(mean_absolute_error(y_test, y_kr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_lasso = [] \n",
    "alpha_space = np.logspace(-10, 0, 20)\n",
    "for r in alpha_space:\n",
    "    lasso = Lasso(alpha= r, normalize=True).fit(X_train_s, y_train)\n",
    "\n",
    "    y_lasso = lasso.predict(X_test_s)\n",
    "\n",
    "    score_lasso.append(mean_absolute_error(y_test, y_lasso))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = [] \n",
    "alpha_space = np.logspace(-4, 0, 20)\n",
    "for r in alpha_space:\n",
    "    ridge = Ridge(alpha= r, normalize=True).fit(X_train_s, y_train)\n",
    "\n",
    "    y_ridge = ridge.predict(X_test_s)\n",
    "\n",
    "    score.append(mean_absolute_error(y_test, y_ridge))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge_cv_scores = cross_val_score(ridge, X_scaled, y_train, cv=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge_cv_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Below we'll run a ridge regression and see how score varies with different alphas. This will show how picking a different alpha score changes the R2. It is generally best not to run past the elbow. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "# Create an array of alphas and lists to store scores\n",
    "alpha_space = np.logspace(-20, 0, 10)\n",
    "ridge_scores = []\n",
    "ridge_scores_std = []\n",
    "\n",
    "# Create a ridge regressor: ridge\n",
    "ridge = Ridge(normalize=True)\n",
    "\n",
    "# Compute scores over range of alphas\n",
    "for alpha in alpha_space:\n",
    "\n",
    "    # Specify the alpha value to use: ridge.alpha\n",
    "    ridge.alpha = alpha\n",
    "    \n",
    "    # Perform 10-fold CV: ridge_cv_scores\n",
    "    ridge_cv_scores = cross_val_score(ridge, X_scaled, y_train, cv=10)\n",
    "    \n",
    "    # Append the mean of ridge_cv_scores to ridge_scores\n",
    "    ridge_scores.append(np.mean(ridge_cv_scores))\n",
    "    \n",
    "    # Append the std of ridge_cv_scores to ridge_scores_std\n",
    "    ridge_scores_std.append(np.std(ridge_cv_scores))\n",
    "\n",
    "# Use this function to create a plot    \n",
    "def display_plot(cv_scores, cv_scores_std):\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(1,1,1)\n",
    "    ax.plot(alpha_space, cv_scores)\n",
    "\n",
    "    std_error = cv_scores_std / np.sqrt(10)\n",
    "\n",
    "    ax.fill_between(alpha_space, cv_scores + std_error, cv_scores - std_error, alpha=0.2)\n",
    "    ax.set_ylabel('CV Score +/- Std Error')\n",
    "    ax.set_xlabel('Alpha')\n",
    "    ax.axhline(np.max(cv_scores), linestyle='--', color='.5')\n",
    "    ax.set_xlim([alpha_space[0], alpha_space[-1]])\n",
    "    ax.set_xscale('log')\n",
    "    plt.show()\n",
    "\n",
    "# Display the plot\n",
    "display_plot(ridge_scores, ridge_scores_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "coef = pd.Series(model_lasso.coef_, index = X_train.columns)\n",
    "print(\"Lasso picked \" + str(sum(coef != 0)) + \" variables and eliminated the other \" +  str(sum(coef == 0)) + \" variables\")\n",
    "\n",
    "\n",
    "\n",
    "imp_coef = pd.concat([coef.sort_values().head(10),\n",
    "                     coef.sort_values().tail(10)])\n",
    "\n",
    "\n",
    "\n",
    "matplotlib.rcParams['figure.figsize'] = (8.0, 10.0)\n",
    "imp_coef.plot(kind = \"barh\")\n",
    "plt.title(\"Coefficients in the Lasso Model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Scaled Data\n",
    "\n",
    "## PCA Data\n",
    "pca = PCA(n_components=410)\n",
    "X_scaled=pca.fit_transform(X_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse_cv(model,X,y):\n",
    "    rmse = np.sqrt(-cross_val_score(model, X, y, scoring=\"neg_mean_squared_error\", cv=5))\n",
    "    return rmse\n",
    "\n",
    "\n",
    "def Standardisation(df, leave_out=None):\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    if leave_out:\n",
    "        leave = df[leave_out]\n",
    "        df = df.drop(leave_out,axis=1)\n",
    "\n",
    "        listed = list(df)\n",
    "        scaler = StandardScaler()\n",
    "        try:\n",
    "            scaled = scaler.fit_transform(df)\n",
    "        except:\n",
    "            print(\"error_triggered\")\n",
    "            scaled = scaler.fit_transform(df.fillna(df.mean()))\n",
    "        df = pd.DataFrame(scaled)\n",
    "        df.columns = listed\n",
    "        df = pd.concat((df,leave ),axis=1)\n",
    "    else:\n",
    "        scaler = StandardScaler()\n",
    "        listed = list(df)\n",
    "        try:\n",
    "            scaled = scaler.fit_transform(df)\n",
    "        except:\n",
    "            print(\"error_triggered\")\n",
    "            scaled = scaler.fit_transform(df.fillna(df.mean()))\n",
    "        df = pd.DataFrame(scaled)\n",
    "        df.columns = listed\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "X_scaled = Standardisation(X_train,[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_scaled.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Machine Learning Benchmarks\n",
    "## In linear models a PCA step can work nicely \n",
    "\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV, KFold\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, ExtraTreesRegressor\n",
    "from sklearn.svm import SVR, LinearSVR\n",
    "from sklearn.linear_model import ElasticNet, SGDRegressor, BayesianRidge\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "models = [LinearRegression(),Ridge(),Lasso(alpha=0.01,max_iter=10000),RandomForestRegressor(),GradientBoostingRegressor(),SVR(),LinearSVR(),\n",
    "          ElasticNet(alpha=0.001,max_iter=10000),SGDRegressor(max_iter=1000,tol=1e-3),BayesianRidge(),KernelRidge(alpha=0.6, kernel='polynomial', degree=2, coef0=2.5),\n",
    "          ExtraTreesRegressor(),XGBRegressor()]\n",
    "\n",
    "names = [\"LR\", \"Ridge\", \"Lasso\", \"RF\", \"GBR\", \"SVR\", \"LinSVR\", \"Ela\",\"SGD\",\"Bay\",\"Ker\",\"Extra\",\"Xgb\"]\n",
    "for name, model in zip(names, models):\n",
    "    score = rmse_cv(model, X_scaled, y_train.reset_index(drop=True))\n",
    "    print(\"{}: {:.6f}, {:.4f}\".format(name,score.mean(),score.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso=Lasso(alpha=0.001)\n",
    "lasso.fit(X_scaled,y_log)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "mean_absolute_error(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_testa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## It has been a long time sinced I used learning curves, and I think it is worth adding one again, and saying something about how much data is needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Learning Curves: If you plot cross-validation (cv) error and training set error rates versus training set size, you can learn a lot. If the two curves approach each other with low error rate, then you are doing well.\n",
    "\n",
    "If it looks like the curves are starting to approach each other and both heading/staying low, then you need more data.\n",
    "\n",
    "If the cv curve remains high, but the training set curve remains low, then you have a high-variance situation. You can either get more data, or use regularization to improve generalization."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py36",
   "language": "python",
   "name": "py36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
